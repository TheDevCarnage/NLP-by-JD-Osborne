{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing  Assignment #1 Fall 2023 \n",
    "## Due September 20th, 2023  11:59pm CST\n",
    "## Student Name:<font color='red'> YOUR NAME HERE </font>\n",
    "Regular Expressions, Tokenization and Normalization, Dynamic Programming, Document Classification, Perplexity\n",
    "\n",
    "* Getting started on cheaha: https://docs.uabgrid.uab.edu/wiki/Cheaha_GettingStarted\n",
    "* Instructions on running Jupyter Notebook on cheaha: https://docs.uabgrid.uab.edu/wiki/Jupyter#Jupyter_on_Cheaha\n",
    "* IPython notebooks: https://ipython.org/ipython-doc/3/notebook/notebook.html#introduction\n",
    "\n",
    "<font color=\"red\">Do not forget to answer the written questions! </font>Just coding will not get you full points. You can answer the questions in the same text box as they are asked.\n",
    "\n",
    "As of the time of writing, a suitable conda environment with NLTK and other needed libraries should be or should be soon available as a Jupyter kernel - \"nlp2022\" or \"DeepPhe\". Ambitious students may still want to learn to create their own environment, but code should work in the provided environment. Instructions for creating your own custom environment on cheaha are provided below:\n",
    "https://docs.uabgrid.uab.edu/wiki/Jupyter#Adding_Custom_Conda_Environments_to_Jupyter\n",
    "\n",
    "Note: I give \"hints\" via imports, but not all imports are shown.\n",
    "\n",
    "Submission will be via Canvas. Attempt all questions, butn not all questions may be marked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. De-identification with Regular Expressions (20 points)\n",
    "\n",
    "You started working for a company that needs to scrub identifiers from user uploaded files in order to comply with EU General Data Protection Regulation (GDPR) . A beta version of a piece of de-identification software used in the United States removed physicians name, patient names and some location names from the note below, but failed to location and some date information. \n",
    "\n",
    "\n",
    "* Write a regular expression/s to replace ALL date and location references with XX/XX/XXXX and XXXXX respectively. \n",
    "\n",
    "* Make use of a dictionary (small set of terms) to assist with this. \n",
    "\n",
    "Your regular expressions should work for this note, but not be so specific that they are unlikely to be useful on other notes. \n",
    "\n",
    "* Print before and after documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import re;\n",
    "fakedoc = \"\"\"\n",
    "                XXXXXX, XXXXXXX\n",
    "                ##########\n",
    "                London Clinic\n",
    "                XXXXX \n",
    "                XXXXX, W1G 6BW\n",
    "                Date 07/01/2023\n",
    "        \n",
    "NEUROLOGY AND PAIN MEDICINE\n",
    "INITIAL OUTPATIENT NOTE\n",
    "\n",
    "XXXXXX, XXXXXX\n",
    "PATIENT HISTORY\n",
    "Ms. XXXXXX is a 56y women seen in consultation at the request of Dr. XXXXXXXXXX. The patient is alert \n",
    "and orinted to self, place and cirumstance. She self-reports stress due the lingering effects of the\n",
    "her 17 year-old son's suicide and has difficulty continuing her work. Denies depression, SI. PMH BPD.\n",
    "\n",
    "Consitution: Well developed, well nourished in no apparent distress.\n",
    "\n",
    "Assessment: Fibromyalgia, stress\n",
    "\n",
    "TREATMENT/PLAN:\n",
    "Duloxetine\n",
    "Suggested finding more time for exercise and sleep for stress\n",
    "\n",
    "REFERRING PHYSICIAN\n",
    "XXXXXX, XXXXXX, MD\n",
    "\n",
    "Electronially signed by XXXXXX, XXXXXX DO, PhD on 07/02/2023\n",
    "\"\"\"\n",
    "print(fakedoc)\n",
    "\n",
    "\n",
    "# Write regular expression/s here, be as general as possible with your regular expression\n",
    "\n",
    "\n",
    "\n",
    "print(fakedoc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. De-identification with ChatGPT and Use of Regex (10 points+5 bonus points)\n",
    "\n",
    "#### Design a prompt to have ChatGPT (GPT-3.5 @ https://chat.openai.com/) do Question 1 for you (2 pts).\n",
    "\n",
    "* Show both the prompt you designed (1 pt) and the output from ChatGPT (1 pt). \n",
    "* Explain why you think ChatGPT does better or worse than your regular expressions in protecting the personal identifying information (PII), personal health information (PHI) and respecting the privacy of this patient. (2 points).\n",
    "\n",
    "\n",
    "#### Name 1 advantage (1 pt) of using ChatGPT for this problem type of problem and 1 disadvantage (1 pt).\n",
    "\n",
    "\n",
    "#### Name 2 disadvantages (2 pts) of regular expressions for this problem.\n",
    "\n",
    "\n",
    "\n",
    "#### BONUS QUESTIONS.  Who do YOU think the person might be? (2 pts). \n",
    "* Ask ChatGPT (with a prompt of your choosing) to identify the person in the note (1 pt). \n",
    "* Give 2 limitations of ChatGPT to explain why it is likely to have difficulty with identifying this person (2 pts).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-tokenization (20 points)\n",
    "GeneRIF (https://www.ncbi.nlm.nih.gov/gene/about-generif) is like Twitter for genes. Biomedical researchers add small free text annotations linking their publication to a particular gene. Each GeneRIF can be considered a document.\n",
    "\n",
    "For this question <font color=\"red\"> DO NOT use ChatGPT for this problem</font> and :\n",
    "* Import a subset of GeneRIF documents, the file is available in canvas and called generifs1000.tsv\n",
    "* Create a corpus, in this case a python list of 1000 geneRIF documents (do not include meta-data in the corpus) in a variable called corpus and print out the number of documents in a corpus (the most basic corpus statistic)\n",
    "* Implement a function that uses the sequential hugging face pre-tokenizer that tokenizes on Whitespace and Punctation. You will use this function again in later questions.\n",
    "* Why is pre-tokenization needed before tokenization?\n",
    "* Print out and save (max1 and max2) distinct geneRIFs that have the highest token count in the data set based on your own whitespace/comma tokenizer. Print out should include token count for both geneRIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import heapq\n",
    "import io\n",
    "import tokenizers.pre_tokenizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization (20 points)\n",
    "\n",
    "Tokenize the GeneRIF corpus using nltk's TweetTokenizer and word_tokenize. Remove stopwords using nltk's English stopwords from both the word_tokenize tokens and the TweetTokenizer tokens. Print out average number of tokens per geneRIF for (1) TweetTokenizer and (2) NLTK word_tokenize. \n",
    "\n",
    "\n",
    "* Write a function to compute the Jacard distance between 2 token sets. Find the single geneRIF/s in the set for which is the most disimilar in terms of Jacard differences between the TweetTokenizer and nltk's workd_tokenize after stopword removal. Print it out.\n",
    "\n",
    "* Which tokenizer is more appropriate and why? Do you think this type of corpus requires a better tokenizer?\n",
    "\n",
    "* Create a new tokenizer by training your own Byte Pair Encoding tokenizer using the pre-tokenizer you wrote in question (3) using geneRIFS100K as training data. Tokenize the largest geneRIF/s using your newly trained tokenizer and comment on the advantages of using Byte Pair Encoding versus traditional tokenization approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KL Divergence (15 Points Available, 5 are Required for all students, remaining 10 are bonus for 662, required for 762)\n",
    "\n",
    "You may use ChatGPT for this question. Create 3 overall corpus-level probability distributions for the tokens in the geneRIF corpus, based on your results from the NLTK's tokenizer, the TweetTokenizer and your own BytePairEncoding Scheme after stopword removal. (5 points) Use native python code only so you understand how this function is computed. \n",
    "\n",
    "Look up KL Divergence in Wikipedia or online. What major pitfall do you anticipate if you use this metric to compare token probability distributions? Compute the KL Divergence (a non-symetric measure) using only native python code (no libraries) between your own BytePairEncoding scheme and the 2 other distributions using LaPlace smoothing. (10 points). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Edit Distance (30 points)\n",
    "Implement the edit distance algorithm described in the textbook in Section 2.17 (Jan 2023 edition, page 24) in a function called Min-Edit-Distance. Do *not* use any available library functions for this but write the implementation in Python yourself from scratch. Use a *substituion cost of 3 and insertion and deletion cost of 1*. \n",
    "\n",
    "\n",
    "Test your code with the examples from class - \"execution\" and \"intention\".\n",
    "\n",
    "Compute the edit distance between the 2 largest distinct GeneRIFs as defined in question 3. Print it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perplexity Question (15 points)\n",
    "You may use ChatGPT for this question, if you do, include your prompt, original generated code as well as your final modified code. Install any needed tokenizers or other tools if they are not included in the starting kernel.\n",
    "\n",
    "Compute the mean perplexity of the GeneRIF corpus using the huggingface evaluate library. Use 3 different libraries: (9 points)\n",
    "* GPT-2\n",
    "* Microsoft's biogpt\n",
    "* BERT base uncased\n",
    "\n",
    "Explain the difference in perplexity between these models, you may need to refer to either the original papers or hugging-face documentation to understand these models. (6 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate\n",
    "#!pip install ipywidgets\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "from transformers import AutoModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7. Document Classification (20 points)\n",
    "ChatGPT is reccomended, but not required for this question. Use git to pull the data set from inside this notebook.  Write a Naive Bayes algorithm in python (without using any 3rd party libraries) to classify SMS messages as Ham or Spam from the csv file at https://github.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/blob/master/spam.csv (10 pts)\n",
    "\n",
    "There are many solutions available for this online, I suggest using ChatGPT but if you use other websites to assist you, do not forget to cite them. Regardless if you use ChatGPT or not, provide an explaination as to what each generated (or written) function does in English. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
